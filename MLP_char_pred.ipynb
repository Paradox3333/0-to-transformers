{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation based on the following paper:\n",
    "\n",
    "# https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key concept: Words are embedded in an n-dimensional embedding space. The MLP learns to organize this space in a way that forms understanding of\n",
    "#              semantics. The goal is to give similar embeddings to similar words. Each embedding dimensions will encode the model's compressed\n",
    "#              understanding. This allows the model to transfer knowledge and generalize to novel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoi = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "itos = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # {} to create a dictionary, i+1 so the starting index is 1, enumerate returns an iterable of tuples\n",
    "print(f'{stoi = }')\n",
    "\n",
    "# Syntax is: {key_expression: value_expression for item in iterable}\n",
    "\n",
    "stoi['.'] = 0 # Add start/end token with 0 index\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(f'{itos = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'),\n",
       " (1, 'b'),\n",
       " (2, 'c'),\n",
       " (3, 'd'),\n",
       " (4, 'e'),\n",
       " (5, 'f'),\n",
       " (6, 'g'),\n",
       " (7, 'h'),\n",
       " (8, 'i'),\n",
       " (9, 'j'),\n",
       " (10, 'k'),\n",
       " (11, 'l'),\n",
       " (12, 'm'),\n",
       " (13, 'n'),\n",
       " (14, 'o'),\n",
       " (15, 'p'),\n",
       " (16, 'q'),\n",
       " (17, 'r'),\n",
       " (18, 's'),\n",
       " (19, 't'),\n",
       " (20, 'u'),\n",
       " (21, 'v'),\n",
       " (22, 'w'),\n",
       " (23, 'x'),\n",
       " (24, 'y'),\n",
       " (25, 'z')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "\n",
    "block_size = 3 # Context length\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words: # run with words[:5] if checking\n",
    "    # print(w)\n",
    "    context = [0] * block_size # Multiplication of a list with a positive integer replicates the contents of the list that many times. \n",
    "                               # here [0, 0, 0]\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), ' ----> ', itos[ix])\n",
    "        \n",
    "        context = context[1:] + [ix] # Crop and append. This essentially shifts the context window to the right.\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape = torch.Size([27, 2]), \n",
      "C.dtype = torch.float32, \n",
      "C = tensor([[ 1.5674, -0.2373],\n",
      "        [-0.0274, -1.1008],\n",
      "        [ 0.2859, -0.0296],\n",
      "        [-1.5471,  0.6049],\n",
      "        [ 0.0791,  0.9046],\n",
      "        [-0.4713,  0.7868],\n",
      "        [-0.3284, -0.4330],\n",
      "        [ 1.3729,  2.9334],\n",
      "        [ 1.5618, -1.6261],\n",
      "        [ 0.6772, -0.8404],\n",
      "        [ 0.9849, -0.1484],\n",
      "        [-1.4795,  0.4483],\n",
      "        [-0.0707,  2.4968],\n",
      "        [ 2.4448, -0.6701],\n",
      "        [-1.2199,  0.3031],\n",
      "        [-1.0725,  0.7276],\n",
      "        [ 0.0511,  1.3095],\n",
      "        [-0.8022, -0.8504],\n",
      "        [-1.8068,  1.2523],\n",
      "        [ 0.1476, -1.0006],\n",
      "        [-0.5030, -1.0660],\n",
      "        [ 0.8480,  2.0275],\n",
      "        [-0.1158, -1.2078],\n",
      "        [-1.0406, -1.5367],\n",
      "        [-0.5132,  0.2961],\n",
      "        [-1.4904, -0.2838],\n",
      "        [ 0.2569,  0.2130]])\n"
     ]
    }
   ],
   "source": [
    "# Create the lookup table \"C\" of embeddings per character\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((27,2), generator=g) # 27 characters embedded in a 2-dimensional space. Returns a tensor.\n",
    "print(f'{C.shape = }, \\n{C.dtype = }, \\n{C = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape # We can index into the tensor C using another tensor.\n",
    "\n",
    "# The shape is (sample x character x embedding)\n",
    "# Remember 1 \"sample\" is made of 3 characters (the preceding 3, to be exact) at that is why the first two dimensions are 32 x 3,\n",
    "# and \"embedding \" consists of 2 values, which is why the last dimension is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C[0] = tensor([ 1.5674, -0.2373])\n",
      "C[X][0,:,0] = tensor([1.5674, 1.5674, 1.5674])\n"
     ]
    }
   ],
   "source": [
    "# Explanation of pytorch advanced indexing\n",
    "\n",
    "# What Happens in C[X]?\n",
    "# When you do C[X], you're not directly indexing C as if C were 3D. Instead, you're using the advanced indexing feature in PyTorch, \n",
    "# where the tensor X acts as a lookup table for rows in C. Here's how it works:\n",
    "\n",
    "# Indexing Mechanism:\n",
    "# X specifies which rows of C to select.\n",
    "# Each ENTRY of X is an index that tells PyTorch which row of C to pick.\n",
    "\n",
    "print(f'{C[0] = }') # This selects the first row of C\n",
    "print(f'{C[X][0,:,0] = }') # This grabs the first embedding for each character in the first sample (sample = context, made of 3 characters)\n",
    "                           # So the 3rd dimension is the useful one which contains the embeddings for each character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hidden layer\n",
    "\n",
    "# Matrix of weights\n",
    "\n",
    "W1 = torch.randn((6, 100), generator=g)  # The NN takes the previous 3 characters and predicts the next one,\n",
    "                            # and each character was embedded using 2 values (2D embedding space).\n",
    "                            # This means that the NN takes (3 characters * 2 embeddings/character = 6 values),\n",
    "                            # and we set the layer to have 100 neurons.\n",
    "                        \n",
    "b1 = torch.randn(100, generator=g)       # Make 100 biases, 1 per neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [-0.4713,  0.7868]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [-0.4713,  0.7868],\n",
       "         [ 2.4448, -0.6701]],\n",
       "\n",
       "        [[-0.4713,  0.7868],\n",
       "         [ 2.4448, -0.6701],\n",
       "         [ 2.4448, -0.6701]],\n",
       "\n",
       "        [[ 2.4448, -0.6701],\n",
       "         [ 2.4448, -0.6701],\n",
       "         [-0.0274, -1.1008]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [-1.0725,  0.7276]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [-1.0725,  0.7276],\n",
       "         [-0.0707,  2.4968]],\n",
       "\n",
       "        [[-1.0725,  0.7276],\n",
       "         [-0.0707,  2.4968],\n",
       "         [ 0.6772, -0.8404]],\n",
       "\n",
       "        [[-0.0707,  2.4968],\n",
       "         [ 0.6772, -0.8404],\n",
       "         [-0.1158, -1.2078]],\n",
       "\n",
       "        [[ 0.6772, -0.8404],\n",
       "         [-0.1158, -1.2078],\n",
       "         [ 0.6772, -0.8404]],\n",
       "\n",
       "        [[-0.1158, -1.2078],\n",
       "         [ 0.6772, -0.8404],\n",
       "         [-0.0274, -1.1008]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [-0.0274, -1.1008]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [-0.0274, -1.1008],\n",
       "         [-0.1158, -1.2078]],\n",
       "\n",
       "        [[-0.0274, -1.1008],\n",
       "         [-0.1158, -1.2078],\n",
       "         [-0.0274, -1.1008]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 0.6772, -0.8404]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 0.6772, -0.8404],\n",
       "         [ 0.1476, -1.0006]],\n",
       "\n",
       "        [[ 0.6772, -0.8404],\n",
       "         [ 0.1476, -1.0006],\n",
       "         [-0.0274, -1.1008]],\n",
       "\n",
       "        [[ 0.1476, -1.0006],\n",
       "         [-0.0274, -1.1008],\n",
       "         [ 0.2859, -0.0296]],\n",
       "\n",
       "        [[-0.0274, -1.1008],\n",
       "         [ 0.2859, -0.0296],\n",
       "         [-0.4713,  0.7868]],\n",
       "\n",
       "        [[ 0.2859, -0.0296],\n",
       "         [-0.4713,  0.7868],\n",
       "         [-0.0707,  2.4968]],\n",
       "\n",
       "        [[-0.4713,  0.7868],\n",
       "         [-0.0707,  2.4968],\n",
       "         [-0.0707,  2.4968]],\n",
       "\n",
       "        [[-0.0707,  2.4968],\n",
       "         [-0.0707,  2.4968],\n",
       "         [-0.0274, -1.1008]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 1.5674, -0.2373],\n",
       "         [ 0.1476, -1.0006]],\n",
       "\n",
       "        [[ 1.5674, -0.2373],\n",
       "         [ 0.1476, -1.0006],\n",
       "         [-1.0725,  0.7276]],\n",
       "\n",
       "        [[ 0.1476, -1.0006],\n",
       "         [-1.0725,  0.7276],\n",
       "         [ 0.0511,  1.3095]],\n",
       "\n",
       "        [[-1.0725,  0.7276],\n",
       "         [ 0.0511,  1.3095],\n",
       "         [ 1.5618, -1.6261]],\n",
       "\n",
       "        [[ 0.0511,  1.3095],\n",
       "         [ 1.5618, -1.6261],\n",
       "         [ 0.6772, -0.8404]],\n",
       "\n",
       "        [[ 1.5618, -1.6261],\n",
       "         [ 0.6772, -0.8404],\n",
       "         [-0.0274, -1.1008]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868],\n",
       "        [ 1.5674, -0.2373, -0.4713,  0.7868,  2.4448, -0.6701],\n",
       "        [-0.4713,  0.7868,  2.4448, -0.6701,  2.4448, -0.6701],\n",
       "        [ 2.4448, -0.6701,  2.4448, -0.6701, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -1.0725,  0.7276],\n",
       "        [ 1.5674, -0.2373, -1.0725,  0.7276, -0.0707,  2.4968],\n",
       "        [-1.0725,  0.7276, -0.0707,  2.4968,  0.6772, -0.8404],\n",
       "        [-0.0707,  2.4968,  0.6772, -0.8404, -0.1158, -1.2078],\n",
       "        [ 0.6772, -0.8404, -0.1158, -1.2078,  0.6772, -0.8404],\n",
       "        [-0.1158, -1.2078,  0.6772, -0.8404, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373, -0.0274, -1.1008, -0.1158, -1.2078],\n",
       "        [-0.0274, -1.1008, -0.1158, -1.2078, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.6772, -0.8404],\n",
       "        [ 1.5674, -0.2373,  0.6772, -0.8404,  0.1476, -1.0006],\n",
       "        [ 0.6772, -0.8404,  0.1476, -1.0006, -0.0274, -1.1008],\n",
       "        [ 0.1476, -1.0006, -0.0274, -1.1008,  0.2859, -0.0296],\n",
       "        [-0.0274, -1.1008,  0.2859, -0.0296, -0.4713,  0.7868],\n",
       "        [ 0.2859, -0.0296, -0.4713,  0.7868, -0.0707,  2.4968],\n",
       "        [-0.4713,  0.7868, -0.0707,  2.4968, -0.0707,  2.4968],\n",
       "        [-0.0707,  2.4968, -0.0707,  2.4968, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.1476, -1.0006],\n",
       "        [ 1.5674, -0.2373,  0.1476, -1.0006, -1.0725,  0.7276],\n",
       "        [ 0.1476, -1.0006, -1.0725,  0.7276,  0.0511,  1.3095],\n",
       "        [-1.0725,  0.7276,  0.0511,  1.3095,  1.5618, -1.6261],\n",
       "        [ 0.0511,  1.3095,  1.5618, -1.6261,  0.6772, -0.8404],\n",
       "        [ 1.5618, -1.6261,  0.6772, -0.8404, -0.0274, -1.1008]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, the layer has to perform a matrix multiplication to operate on the inputs:\n",
    "\n",
    "# Something like emb @ W1 + b1\n",
    "\n",
    "# But this can't be done because the dimensions of emb and W1 are not compatible. Therefore, we need to reshape emb e.g. through concatenation.\n",
    "\n",
    "\n",
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)    # This grabs the embeddings of the 1st char, 2nd char, and 3rd char\n",
    "                                                            # and adds them as columns (dim 1)\n",
    "\n",
    "# The problem is that this code does not generalize because we have to manually list the tensors in the list that is passed to cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868],\n",
       "        [ 1.5674, -0.2373, -0.4713,  0.7868,  2.4448, -0.6701],\n",
       "        [-0.4713,  0.7868,  2.4448, -0.6701,  2.4448, -0.6701],\n",
       "        [ 2.4448, -0.6701,  2.4448, -0.6701, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -1.0725,  0.7276],\n",
       "        [ 1.5674, -0.2373, -1.0725,  0.7276, -0.0707,  2.4968],\n",
       "        [-1.0725,  0.7276, -0.0707,  2.4968,  0.6772, -0.8404],\n",
       "        [-0.0707,  2.4968,  0.6772, -0.8404, -0.1158, -1.2078],\n",
       "        [ 0.6772, -0.8404, -0.1158, -1.2078,  0.6772, -0.8404],\n",
       "        [-0.1158, -1.2078,  0.6772, -0.8404, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373, -0.0274, -1.1008, -0.1158, -1.2078],\n",
       "        [-0.0274, -1.1008, -0.1158, -1.2078, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.6772, -0.8404],\n",
       "        [ 1.5674, -0.2373,  0.6772, -0.8404,  0.1476, -1.0006],\n",
       "        [ 0.6772, -0.8404,  0.1476, -1.0006, -0.0274, -1.1008],\n",
       "        [ 0.1476, -1.0006, -0.0274, -1.1008,  0.2859, -0.0296],\n",
       "        [-0.0274, -1.1008,  0.2859, -0.0296, -0.4713,  0.7868],\n",
       "        [ 0.2859, -0.0296, -0.4713,  0.7868, -0.0707,  2.4968],\n",
       "        [-0.4713,  0.7868, -0.0707,  2.4968, -0.0707,  2.4968],\n",
       "        [-0.0707,  2.4968, -0.0707,  2.4968, -0.0274, -1.1008],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
       "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.1476, -1.0006],\n",
       "        [ 1.5674, -0.2373,  0.1476, -1.0006, -1.0725,  0.7276],\n",
       "        [ 0.1476, -1.0006, -1.0725,  0.7276,  0.0511,  1.3095],\n",
       "        [-1.0725,  0.7276,  0.0511,  1.3095,  1.5618, -1.6261],\n",
       "        [ 0.0511,  1.3095,  1.5618, -1.6261,  0.6772, -0.8404],\n",
       "        [ 1.5618, -1.6261,  0.6772, -0.8404, -0.0274, -1.1008]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better way with torch.unbind(emb, 1), which is equal to [emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]] but generalizes.\n",
    "\n",
    "torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]), \n",
      "a.shape = torch.Size([18]), \n",
      "\n",
      "a.storage() =  0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      " 14\n",
      " 15\n",
      " 16\n",
      " 17\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emman\\AppData\\Local\\Temp\\ipykernel_13288\\592617819.py:6: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f'{a = }, \\n{a.shape = }, \\n\\n{a.storage() = }') # Note: a.storage() returns a TypedStorage object, which is deprecated. Use .untyped_storage()\n"
     ]
    }
   ],
   "source": [
    "# But there is actually a more efficient way.\n",
    "\n",
    "# Proof:\n",
    "\n",
    "a = torch.arange(18)\n",
    "print(f'{a = }, \\n{a.shape = }, \\n\\n{a.storage() = }') # Note: a.storage() returns a TypedStorage object, which is deprecated. Use .untyped_storage()\n",
    "\n",
    "# Note: In computer memory the data of a tensor is always represented as a 1D vector!\n",
    "#       Pytorch then interprets this number sequence as a tensor of specific dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.view(3,3,2) = tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]]])\n",
      "\n",
      "emb.view(32, 6) = tensor([[ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.4713,  0.7868],\n",
      "        [ 1.5674, -0.2373, -0.4713,  0.7868,  2.4448, -0.6701],\n",
      "        [-0.4713,  0.7868,  2.4448, -0.6701,  2.4448, -0.6701],\n",
      "        [ 2.4448, -0.6701,  2.4448, -0.6701, -0.0274, -1.1008],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373, -1.0725,  0.7276],\n",
      "        [ 1.5674, -0.2373, -1.0725,  0.7276, -0.0707,  2.4968],\n",
      "        [-1.0725,  0.7276, -0.0707,  2.4968,  0.6772, -0.8404],\n",
      "        [-0.0707,  2.4968,  0.6772, -0.8404, -0.1158, -1.2078],\n",
      "        [ 0.6772, -0.8404, -0.1158, -1.2078,  0.6772, -0.8404],\n",
      "        [-0.1158, -1.2078,  0.6772, -0.8404, -0.0274, -1.1008],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373, -0.0274, -1.1008],\n",
      "        [ 1.5674, -0.2373, -0.0274, -1.1008, -0.1158, -1.2078],\n",
      "        [-0.0274, -1.1008, -0.1158, -1.2078, -0.0274, -1.1008],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.6772, -0.8404],\n",
      "        [ 1.5674, -0.2373,  0.6772, -0.8404,  0.1476, -1.0006],\n",
      "        [ 0.6772, -0.8404,  0.1476, -1.0006, -0.0274, -1.1008],\n",
      "        [ 0.1476, -1.0006, -0.0274, -1.1008,  0.2859, -0.0296],\n",
      "        [-0.0274, -1.1008,  0.2859, -0.0296, -0.4713,  0.7868],\n",
      "        [ 0.2859, -0.0296, -0.4713,  0.7868, -0.0707,  2.4968],\n",
      "        [-0.4713,  0.7868, -0.0707,  2.4968, -0.0707,  2.4968],\n",
      "        [-0.0707,  2.4968, -0.0707,  2.4968, -0.0274, -1.1008],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373],\n",
      "        [ 1.5674, -0.2373,  1.5674, -0.2373,  0.1476, -1.0006],\n",
      "        [ 1.5674, -0.2373,  0.1476, -1.0006, -1.0725,  0.7276],\n",
      "        [ 0.1476, -1.0006, -1.0725,  0.7276,  0.0511,  1.3095],\n",
      "        [-1.0725,  0.7276,  0.0511,  1.3095,  1.5618, -1.6261],\n",
      "        [ 0.0511,  1.3095,  1.5618, -1.6261,  0.6772, -0.8404],\n",
      "        [ 1.5618, -1.6261,  0.6772, -0.8404, -0.0274, -1.1008]])\n"
     ]
    }
   ],
   "source": [
    "# On the other hand,\n",
    "\n",
    "print(f'{a.view(3,3,2) = }') # a.view(9, 9), a.view(6, 3), etc.\n",
    "\n",
    "#  .view() is therefore more efficient than .cat() in PyTorch because .view() only alters the shape of the tensor by modifying its metadata, \n",
    "#  without changing the underlying data. This operation is computationally inexpensive since no new memory allocation occurs. \n",
    "#  In contrast, .cat() creates a new tensor by concatenating existing tensors, which involves copying data and allocating new memory, \n",
    "#  making it more resource-intensive.\n",
    "\n",
    "# So instead we can do:\n",
    "\n",
    "print(f'\\n{emb.view(32, 6) = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6952e+00,  8.5502e+00,  1.6284e+00,  ...,  2.2642e+00,\n",
       "         -1.9505e-01,  1.8469e+00],\n",
       "        [ 2.8741e-01,  4.3343e+00,  1.0142e+00,  ...,  2.8221e+00,\n",
       "          3.9128e+00,  3.4733e+00],\n",
       "        [-3.1026e+00,  9.9601e+00, -1.3306e+00,  ..., -5.7069e-01,\n",
       "         -5.9107e+00, -6.9120e-03],\n",
       "        ...,\n",
       "        [-4.3248e+00,  7.4938e+00, -1.6386e+00,  ..., -5.1557e+00,\n",
       "         -3.3276e+00, -3.2464e+00],\n",
       "        [-1.4951e+00,  5.6195e+00,  2.5079e+00,  ..., -1.0607e+00,\n",
       "         -5.2543e-01,  3.4893e+00],\n",
       "        [-1.4982e+00,  8.5941e+00,  1.8897e+00,  ...,  2.4983e+00,\n",
       "          6.9596e+00,  2.6822e+00]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform hidden layer operation\n",
    "\n",
    "# emb.view(32, 6) @ W1 + b1\n",
    "emb.view(emb.shape[0], 6) @ W1 + b1     # Even better so dims are not hardcoded.\n",
    "                                        # emb.shape[0] selects the first dimension given by emb.shape.\n",
    "                                        # Can also use -1 as the first dim so Pytorch derives it itself, knowing that the number of elements has to\n",
    "                                        # be equal to the original tensor's.\n",
    "\n",
    "# Corresponds to the logits of the hidden layer, pre-activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = tensor([[-0.9348,  1.0000,  0.9258,  ...,  0.9786, -0.1926,  0.9515],\n",
      "        [ 0.2797,  0.9997,  0.7675,  ...,  0.9929,  0.9992,  0.9981],\n",
      "        [-0.9960,  1.0000, -0.8694,  ..., -0.5159, -1.0000, -0.0069],\n",
      "        ...,\n",
      "        [-0.9996,  1.0000, -0.9273,  ..., -0.9999, -0.9974, -0.9970],\n",
      "        [-0.9043,  1.0000,  0.9868,  ..., -0.7859, -0.4819,  0.9981],\n",
      "        [-0.9048,  1.0000,  0.9553,  ...,  0.9866,  1.0000,  0.9907]]), \n",
      "\n",
      "h.shape = torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "# Calculate activations\n",
    "\n",
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "\n",
    "print(f'{h = }, \\n\\n{h.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(emb.view(emb.shape[0], 6) @ W1).shape = torch.Size([32, 100])\n",
      "\n",
      "b1.shape = torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting correctness check:\n",
    "\n",
    "print(f'{(emb.view(emb.shape[0], 6) @ W1).shape = }')\n",
    "print(f'\\n{b1.shape = }')\n",
    "\n",
    "# So internally the broadcasting will\n",
    "\n",
    "# 32, 100\n",
    "#     100\n",
    "\n",
    "# first align the dimensions from the right,\n",
    "\n",
    "# 32, 100\n",
    "#  1, 100\n",
    "\n",
    "# then add a dimension made of 1s entries (this turns b1 into a row vector),\n",
    "# and then this row vector will be copied vertically to match the 32, 100 dim necessary for the addition\n",
    "\n",
    "# Conclusion: broadcasting in this case achieves the desired operation as the element-wise addition is performed with the same values for every row,\n",
    "# meaning each neuron (dim 1) adds the same bias to each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output layer\n",
    "\n",
    "W2 = torch.randn((100, 27), generator=g) # 100 input values from the previous layer (1 per neuron), and 27 outputs (a probability distribution over 27 characters)\n",
    "b2 = torch.randn(27, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape = torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "print(f'{logits.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement softmax fxn\n",
    "\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True) # Reminder: summing along dim 1 means summing 1 row across all it's columns (so it's a row sum)\n",
    "                                            # Reminder: keepdim=True necessary otherwise 1 dimension is collapsed, messing with broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape # 1 label character per training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5213e-14, 1.2830e-12, 1.9647e-08, 3.1758e-10, 5.6763e-12, 1.0823e-10,\n",
       "        1.8821e-14, 1.1087e-08, 1.6134e-09, 2.1917e-03, 5.3863e-08, 3.1970e-04,\n",
       "        2.0283e-10, 3.5709e-11, 6.2336e-07, 5.1704e-07, 1.4206e-01, 9.5657e-09,\n",
       "        2.0670e-09, 2.5181e-02, 7.6846e-05, 2.8706e-12, 1.6961e-09, 5.6464e-15,\n",
       "        4.4656e-03, 2.6851e-09, 3.5864e-05, 2.3389e-04, 1.6890e-09, 9.5614e-01,\n",
       "        9.7404e-10, 2.1230e-12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the probabilities assigned by the NN to the correct characters (Y) following each sample sequence.\n",
    "\n",
    "prob[torch.arange(32), Y] # This provides two iterators of indices (tensors), since Pytorch supports advanced indexing there is no need to zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of total parameters\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "sum(p.nelement() for p in parameters)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(17.7697)\n"
     ]
    }
   ],
   "source": [
    "# Loss function - Negative log likelihood\n",
    "\n",
    "# Manual NLL:\n",
    "\n",
    "# loss = -prob[torch.arange(32), Y].log().mean() # \n",
    "\n",
    "loss = F.cross_entropy(logits, Y)   # Cross entropy loss is just NLL + built in softmax,\n",
    "                                    # but more efficient since Pytorch uses a fused kernels to do all the operations and so uses less memory and time.\n",
    "                                    # Additionally, backprop becomes more efficient because the derivatives of fused operations are analytically simpler\n",
    "\n",
    "print(f'{loss = }')\n",
    "\n",
    "# Also, manual implementation runs into this problem:\n",
    " \n",
    "# logits = torch.tensor([-100, -3, 0, 100]) # large positive number exceeds the range of the floating point datatype, resulting in \"inf\"\n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum()\n",
    "# probs\n",
    "\n",
    "# The way Pytorch solves this internally is that it scales the logits down by subtracting the highest logit, since this does not change the\n",
    "# probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([228146, 3]), \n",
      "Y.shape = torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "print(f'{X.shape = }, \\n{Y.shape = }') # Check data dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn(27, 2, generator=g)\n",
    "W1 = torch.randn(6, 100, generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn(100, 27, generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters)  # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters: # Pytorch requirement\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.444650650024414\n",
      "16.026756286621094\n",
      "19.595340728759766\n",
      "14.392146110534668\n",
      "16.09246253967285\n",
      "12.661137580871582\n",
      "13.228506088256836\n",
      "11.690152168273926\n",
      "12.289734840393066\n",
      "10.544520378112793\n",
      "9.81041431427002\n",
      "9.253498077392578\n",
      "10.30218505859375\n",
      "9.657097816467285\n",
      "10.381988525390625\n",
      "8.897500038146973\n",
      "8.594795227050781\n",
      "8.87955093383789\n",
      "6.760862350463867\n",
      "8.90519905090332\n",
      "9.463692665100098\n",
      "7.72509765625\n",
      "6.688868522644043\n",
      "6.6442131996154785\n",
      "6.783695220947266\n",
      "8.885218620300293\n",
      "7.291264533996582\n",
      "6.676371097564697\n",
      "7.230817794799805\n",
      "6.5191826820373535\n",
      "7.807373523712158\n",
      "7.735731601715088\n",
      "8.524930953979492\n",
      "5.371830463409424\n",
      "7.295154571533203\n",
      "6.7521514892578125\n",
      "6.151376247406006\n",
      "7.431678771972656\n",
      "5.562653541564941\n",
      "7.038995265960693\n",
      "5.1658830642700195\n",
      "4.6005449295043945\n",
      "6.156202793121338\n",
      "5.665262222290039\n",
      "5.118688583374023\n",
      "4.969459533691406\n",
      "7.730938911437988\n",
      "4.054111003875732\n",
      "4.814337253570557\n",
      "7.172355651855469\n",
      "6.0982890129089355\n",
      "4.236999034881592\n",
      "5.804749011993408\n",
      "5.7193379402160645\n",
      "5.803781986236572\n",
      "4.994479656219482\n",
      "5.5395588874816895\n",
      "4.3998260498046875\n",
      "4.6351318359375\n",
      "5.167760372161865\n",
      "5.7727437019348145\n",
      "5.532530784606934\n",
      "6.490377426147461\n",
      "4.899216651916504\n",
      "4.620826244354248\n",
      "5.391972064971924\n",
      "4.872256278991699\n",
      "4.191134452819824\n",
      "5.345571994781494\n",
      "3.6602444648742676\n",
      "5.471292972564697\n",
      "3.966306686401367\n",
      "3.8094050884246826\n",
      "3.402458906173706\n",
      "3.905871868133545\n",
      "3.9656918048858643\n",
      "4.848133087158203\n",
      "5.3440260887146\n",
      "3.362257719039917\n",
      "4.182471752166748\n",
      "4.377389430999756\n",
      "4.034074783325195\n",
      "4.378621578216553\n",
      "4.309510707855225\n",
      "4.101109027862549\n",
      "3.4685397148132324\n",
      "4.091853141784668\n",
      "3.9747371673583984\n",
      "3.4100124835968018\n",
      "3.928934335708618\n",
      "3.8598403930664062\n",
      "3.421248435974121\n",
      "3.901937484741211\n",
      "3.821150541305542\n",
      "3.931145191192627\n",
      "3.805879831314087\n",
      "3.9833292961120605\n",
      "3.0526442527770996\n",
      "3.074199914932251\n",
      "3.050187587738037\n",
      "4.6316914558410645\n",
      "3.3014814853668213\n",
      "3.3413257598876953\n",
      "3.0781872272491455\n",
      "4.394444942474365\n",
      "3.852435350418091\n",
      "3.7824392318725586\n",
      "3.4356281757354736\n",
      "2.909881591796875\n",
      "5.461898326873779\n",
      "3.390960693359375\n",
      "5.367862701416016\n",
      "3.068598747253418\n",
      "4.200375080108643\n",
      "3.63751482963562\n",
      "3.2351698875427246\n",
      "3.1379706859588623\n",
      "3.6532676219940186\n",
      "3.6838419437408447\n",
      "3.6113882064819336\n",
      "3.4135396480560303\n",
      "2.776960849761963\n",
      "3.5616331100463867\n",
      "2.8761894702911377\n",
      "2.603267192840576\n",
      "3.130742073059082\n",
      "3.4310836791992188\n",
      "2.8755955696105957\n",
      "2.8099966049194336\n",
      "2.6340279579162598\n",
      "3.705052614212036\n",
      "2.5675129890441895\n",
      "3.6588823795318604\n",
      "4.450950622558594\n",
      "3.587397336959839\n",
      "2.897444725036621\n",
      "3.111884355545044\n",
      "2.555135488510132\n",
      "4.709339141845703\n",
      "3.6902918815612793\n",
      "3.5934183597564697\n",
      "3.3778269290924072\n",
      "3.8812966346740723\n",
      "4.274111270904541\n",
      "3.7355875968933105\n",
      "3.076263189315796\n",
      "3.5222175121307373\n",
      "3.0705626010894775\n",
      "3.5293359756469727\n",
      "3.926297664642334\n",
      "3.758432388305664\n",
      "3.4526290893554688\n",
      "3.6378190517425537\n",
      "3.2685887813568115\n",
      "2.7939765453338623\n",
      "3.037839889526367\n",
      "3.2646031379699707\n",
      "2.834163188934326\n",
      "3.86824893951416\n",
      "3.1184990406036377\n",
      "2.716944694519043\n",
      "2.9649932384490967\n",
      "2.8335535526275635\n",
      "2.852210760116577\n",
      "3.0710697174072266\n",
      "2.8282008171081543\n",
      "2.946566581726074\n",
      "3.5614662170410156\n",
      "2.894644260406494\n",
      "3.1311593055725098\n",
      "3.35101580619812\n",
      "2.594172477722168\n",
      "3.545245409011841\n",
      "3.288703680038452\n",
      "3.5714316368103027\n",
      "3.638292074203491\n",
      "2.775484561920166\n",
      "3.769519805908203\n",
      "3.661102771759033\n",
      "3.2927703857421875\n",
      "3.5302367210388184\n",
      "3.6034984588623047\n",
      "2.998098373413086\n",
      "2.747225761413574\n",
      "3.1229641437530518\n",
      "3.707798719406128\n",
      "3.1687755584716797\n",
      "2.890627861022949\n",
      "2.996243715286255\n",
      "3.1771793365478516\n",
      "3.147609233856201\n",
      "3.217397928237915\n",
      "4.1880388259887695\n",
      "3.550858974456787\n",
      "3.6586315631866455\n",
      "3.4288196563720703\n",
      "2.831895589828491\n",
      "3.4690322875976562\n",
      "3.5725808143615723\n",
      "3.19736385345459\n",
      "2.949550151824951\n",
      "2.689305543899536\n",
      "2.9801814556121826\n",
      "3.0708255767822266\n",
      "3.2578089237213135\n",
      "2.942553758621216\n",
      "3.8298888206481934\n",
      "3.0580966472625732\n",
      "2.7536656856536865\n",
      "3.2590479850769043\n",
      "2.7960259914398193\n",
      "2.5355260372161865\n",
      "2.7826664447784424\n",
      "2.8251965045928955\n",
      "3.059497594833374\n",
      "2.912050485610962\n",
      "2.7885308265686035\n",
      "2.7091822624206543\n",
      "3.4817728996276855\n",
      "2.9349207878112793\n",
      "3.1671950817108154\n",
      "2.8634567260742188\n",
      "2.7178332805633545\n",
      "2.9000720977783203\n",
      "2.461055040359497\n",
      "2.8156604766845703\n",
      "3.2331466674804688\n",
      "3.212130546569824\n",
      "3.2702579498291016\n",
      "2.940023422241211\n",
      "3.135611057281494\n",
      "3.009843587875366\n",
      "3.013232469558716\n",
      "2.749307870864868\n",
      "2.976031541824341\n",
      "3.029350519180298\n",
      "2.463801622390747\n",
      "2.93166184425354\n",
      "3.1870288848876953\n",
      "2.7658188343048096\n",
      "2.8217267990112305\n",
      "3.6300015449523926\n",
      "2.6266398429870605\n",
      "3.4785830974578857\n",
      "2.8382973670959473\n",
      "3.192497968673706\n",
      "2.536980152130127\n",
      "2.579378366470337\n",
      "2.8267204761505127\n",
      "3.3196356296539307\n",
      "3.5101280212402344\n",
      "3.2073793411254883\n",
      "2.5196197032928467\n",
      "3.4093704223632812\n",
      "2.5971932411193848\n",
      "2.658674478530884\n",
      "3.3637168407440186\n",
      "2.6517462730407715\n",
      "2.787506341934204\n",
      "2.9002344608306885\n",
      "2.8786251544952393\n",
      "3.5907225608825684\n",
      "3.17400860786438\n",
      "2.892408609390259\n",
      "3.3110601902008057\n",
      "2.642122745513916\n",
      "2.3720037937164307\n",
      "2.5156033039093018\n",
      "3.337425947189331\n",
      "2.820681571960449\n",
      "3.116151809692383\n",
      "3.0291669368743896\n",
      "2.9022457599639893\n",
      "3.0061910152435303\n",
      "2.730436325073242\n",
      "2.837874412536621\n",
      "2.920964002609253\n",
      "2.9767963886260986\n",
      "2.925917863845825\n",
      "2.9149036407470703\n",
      "2.845276355743408\n",
      "2.442572593688965\n",
      "2.4464197158813477\n",
      "3.4235010147094727\n",
      "2.9973599910736084\n",
      "2.90388822555542\n",
      "3.485663414001465\n",
      "3.3829903602600098\n",
      "3.117284059524536\n",
      "2.673003911972046\n",
      "2.8410935401916504\n",
      "2.292989492416382\n",
      "2.380439519882202\n",
      "3.1481430530548096\n",
      "3.6973226070404053\n",
      "2.686901092529297\n",
      "3.301116466522217\n",
      "2.7198078632354736\n",
      "2.6815173625946045\n",
      "2.577885627746582\n",
      "3.1469035148620605\n",
      "3.323432445526123\n",
      "3.4460535049438477\n",
      "2.918311834335327\n",
      "2.9981353282928467\n",
      "3.7130086421966553\n",
      "2.4330928325653076\n",
      "2.708831787109375\n",
      "2.9535837173461914\n",
      "2.8690061569213867\n",
      "2.8431499004364014\n",
      "3.3200876712799072\n",
      "2.598470687866211\n",
      "2.2388439178466797\n",
      "2.638504981994629\n",
      "2.6556131839752197\n",
      "2.839942216873169\n",
      "2.70025372505188\n",
      "3.2812414169311523\n",
      "2.8250224590301514\n",
      "2.8996500968933105\n",
      "2.690561532974243\n",
      "3.4972739219665527\n",
      "3.155475378036499\n",
      "2.758310556411743\n",
      "2.8751654624938965\n",
      "2.655177593231201\n",
      "2.7922539710998535\n",
      "2.809164524078369\n",
      "3.2164981365203857\n",
      "3.161484956741333\n",
      "2.8453874588012695\n",
      "2.9630038738250732\n",
      "2.9130935668945312\n",
      "3.094069242477417\n",
      "2.950716733932495\n",
      "2.9738452434539795\n",
      "3.1512584686279297\n",
      "2.944613218307495\n",
      "2.6835153102874756\n",
      "3.1467349529266357\n",
      "2.705500841140747\n",
      "3.046449661254883\n",
      "2.625497817993164\n",
      "2.5745372772216797\n",
      "2.452009677886963\n",
      "2.860905647277832\n",
      "3.082644462585449\n",
      "2.3419907093048096\n",
      "2.7153587341308594\n",
      "2.5121777057647705\n",
      "3.2235000133514404\n",
      "2.8737940788269043\n",
      "2.7287135124206543\n",
      "2.633272886276245\n",
      "2.665964365005493\n",
      "3.193917751312256\n",
      "3.136636972427368\n",
      "3.158017158508301\n",
      "2.948378801345825\n",
      "2.5574073791503906\n",
      "2.945625066757202\n",
      "2.7867109775543213\n",
      "3.388998031616211\n",
      "2.746814250946045\n",
      "2.750882625579834\n",
      "2.6903786659240723\n",
      "2.8402469158172607\n",
      "2.435483455657959\n",
      "2.956298589706421\n",
      "3.04008150100708\n",
      "2.519151210784912\n",
      "2.765580415725708\n",
      "2.9651670455932617\n",
      "2.6258087158203125\n",
      "2.9744362831115723\n",
      "2.5728392601013184\n",
      "2.884498119354248\n",
      "2.910712242126465\n",
      "2.4508838653564453\n",
      "3.345473289489746\n",
      "3.1443183422088623\n",
      "2.706733465194702\n",
      "2.8506853580474854\n",
      "2.607722759246826\n",
      "3.191112756729126\n",
      "2.799952745437622\n",
      "2.756803035736084\n",
      "2.9640755653381348\n",
      "2.289280891418457\n",
      "2.9448211193084717\n",
      "2.286167621612549\n",
      "2.98416805267334\n",
      "2.9187052249908447\n",
      "2.870450019836426\n",
      "2.943328857421875\n",
      "3.7757718563079834\n",
      "2.643800973892212\n",
      "2.8759028911590576\n",
      "3.121783971786499\n",
      "2.84253191947937\n",
      "2.691188335418701\n",
      "2.9247543811798096\n",
      "3.535972833633423\n",
      "2.7847187519073486\n",
      "3.128429412841797\n",
      "2.788181781768799\n",
      "2.3805599212646484\n",
      "3.1219797134399414\n",
      "2.570033550262451\n",
      "2.5045878887176514\n",
      "3.077364921569824\n",
      "2.261556386947632\n",
      "2.882708787918091\n",
      "2.421938180923462\n",
      "2.7617685794830322\n",
      "2.470552682876587\n",
      "2.7311480045318604\n",
      "2.8783280849456787\n",
      "2.9684269428253174\n",
      "2.9031004905700684\n",
      "2.520089626312256\n",
      "3.0027425289154053\n",
      "2.6034696102142334\n",
      "3.115276575088501\n",
      "2.4729294776916504\n",
      "3.0143051147460938\n",
      "3.018771171569824\n",
      "2.500731945037842\n",
      "3.2774789333343506\n",
      "2.6674556732177734\n",
      "2.6739015579223633\n",
      "2.7376937866210938\n",
      "2.6635007858276367\n",
      "2.8453636169433594\n",
      "2.385174036026001\n",
      "2.6910550594329834\n",
      "2.611262321472168\n",
      "2.820448875427246\n",
      "2.418677568435669\n",
      "2.7489898204803467\n",
      "3.095397710800171\n",
      "2.828770399093628\n",
      "3.082292318344116\n",
      "2.5973641872406006\n",
      "2.904588460922241\n",
      "2.5793614387512207\n",
      "3.1071481704711914\n",
      "3.2456607818603516\n",
      "2.6621954441070557\n",
      "2.857586145401001\n",
      "2.7434985637664795\n",
      "2.749948501586914\n",
      "2.850072145462036\n",
      "2.838883876800537\n",
      "3.0049500465393066\n",
      "2.887524127960205\n",
      "2.90020751953125\n",
      "2.4571406841278076\n",
      "3.5664443969726562\n",
      "2.480525255203247\n",
      "2.861642599105835\n",
      "2.836595058441162\n",
      "3.1030006408691406\n",
      "2.8696563243865967\n",
      "2.7853407859802246\n",
      "2.601771354675293\n",
      "2.793210029602051\n",
      "2.701596736907959\n",
      "2.6527256965637207\n",
      "2.7828733921051025\n",
      "2.695712089538574\n",
      "2.981234312057495\n",
      "2.8241219520568848\n",
      "2.69368577003479\n",
      "3.03529953956604\n",
      "3.0257842540740967\n",
      "2.825441598892212\n",
      "2.417036771774292\n",
      "2.871159315109253\n",
      "2.302061080932617\n",
      "3.246715784072876\n",
      "2.7237274646759033\n",
      "2.8147377967834473\n",
      "2.9319138526916504\n",
      "2.9737257957458496\n",
      "3.3547744750976562\n",
      "3.020742654800415\n",
      "2.52732515335083\n",
      "2.5648300647735596\n",
      "3.1089999675750732\n",
      "3.062826633453369\n",
      "2.557678699493408\n",
      "3.077641248703003\n",
      "2.603044033050537\n",
      "2.8671865463256836\n",
      "2.72074818611145\n",
      "2.8735506534576416\n",
      "2.422614336013794\n",
      "2.665785789489746\n",
      "3.1350154876708984\n",
      "2.76651930809021\n",
      "2.6319518089294434\n",
      "2.539090156555176\n",
      "2.560936689376831\n",
      "2.077627182006836\n",
      "2.499249219894409\n",
      "2.8809497356414795\n",
      "3.083376169204712\n",
      "2.981456756591797\n",
      "2.647325277328491\n",
      "2.937755584716797\n",
      "3.0412309169769287\n",
      "2.621324062347412\n",
      "3.3199498653411865\n",
      "2.6631689071655273\n",
      "2.7175443172454834\n",
      "2.577826499938965\n",
      "2.7460503578186035\n",
      "2.541990280151367\n",
      "2.6422295570373535\n",
      "2.5588653087615967\n",
      "2.8110592365264893\n",
      "2.6385111808776855\n",
      "2.7836623191833496\n",
      "2.611539363861084\n",
      "2.9528579711914062\n",
      "2.9082932472229004\n",
      "2.43992280960083\n",
      "3.011613607406616\n",
      "2.446990728378296\n",
      "3.048611879348755\n",
      "3.097137928009033\n",
      "2.6747357845306396\n",
      "2.8040225505828857\n",
      "2.4241786003112793\n",
      "2.924417018890381\n",
      "2.7368876934051514\n",
      "2.9410972595214844\n",
      "2.9235317707061768\n",
      "2.5878825187683105\n",
      "3.208627223968506\n",
      "2.7462878227233887\n",
      "2.7250375747680664\n",
      "2.786661386489868\n",
      "2.6457033157348633\n",
      "2.539179801940918\n",
      "2.721580982208252\n",
      "3.1306371688842773\n",
      "2.522129535675049\n",
      "2.6784348487854004\n",
      "2.436555862426758\n",
      "2.9407896995544434\n",
      "2.3708395957946777\n",
      "3.290489912033081\n",
      "2.5376060009002686\n",
      "3.405369758605957\n",
      "2.5758934020996094\n",
      "2.939250946044922\n",
      "2.788285732269287\n",
      "2.3222734928131104\n",
      "2.6481752395629883\n",
      "2.7350375652313232\n",
      "2.7406859397888184\n",
      "2.5812876224517822\n",
      "3.085819959640503\n",
      "2.947409152984619\n",
      "3.076780080795288\n",
      "2.6782569885253906\n",
      "2.9652624130249023\n",
      "2.6304938793182373\n",
      "2.4559061527252197\n",
      "2.7644617557525635\n",
      "2.799232244491577\n",
      "3.3639023303985596\n",
      "2.595743417739868\n",
      "2.6247458457946777\n",
      "2.3784327507019043\n",
      "2.4322047233581543\n",
      "2.7588353157043457\n",
      "2.78725004196167\n",
      "2.6283020973205566\n",
      "3.074568271636963\n",
      "2.473611831665039\n",
      "2.408327102661133\n",
      "2.928638219833374\n",
      "2.618067741394043\n",
      "2.5383872985839844\n",
      "2.5899970531463623\n",
      "2.408625602722168\n",
      "2.365122079849243\n",
      "2.8015880584716797\n",
      "2.4094016551971436\n",
      "2.9372634887695312\n",
      "2.222508430480957\n",
      "2.751997232437134\n",
      "3.193103075027466\n",
      "2.5814127922058105\n",
      "2.8719358444213867\n",
      "2.78277587890625\n",
      "2.7507686614990234\n",
      "2.6612372398376465\n",
      "2.6985530853271484\n",
      "3.1099131107330322\n",
      "2.936267852783203\n",
      "2.873976707458496\n",
      "2.997907876968384\n",
      "2.6682119369506836\n",
      "3.0186257362365723\n",
      "2.7316575050354004\n",
      "2.6250975131988525\n",
      "2.57574725151062\n",
      "2.980050563812256\n",
      "2.5406930446624756\n",
      "3.175650119781494\n",
      "3.490100145339966\n",
      "2.8967108726501465\n",
      "2.980405807495117\n",
      "2.63505482673645\n",
      "3.049400568008423\n",
      "2.637970447540283\n",
      "2.9059581756591797\n",
      "2.9889543056488037\n",
      "2.619868278503418\n",
      "3.3536274433135986\n",
      "2.987520933151245\n",
      "2.728621006011963\n",
      "3.079942464828491\n",
      "3.1602306365966797\n",
      "2.6273059844970703\n",
      "2.8595330715179443\n",
      "2.4625754356384277\n",
      "3.8437280654907227\n",
      "3.3282999992370605\n",
      "3.17081618309021\n",
      "2.7224600315093994\n",
      "2.512117624282837\n",
      "2.4659552574157715\n",
      "2.619903564453125\n",
      "2.4955978393554688\n",
      "2.558962106704712\n",
      "2.4741339683532715\n",
      "2.566883087158203\n",
      "2.203557014465332\n",
      "3.017091989517212\n",
      "2.640097141265869\n",
      "2.872551441192627\n",
      "2.8339269161224365\n",
      "2.779697895050049\n",
      "2.705601215362549\n",
      "3.026749849319458\n",
      "2.9132328033447266\n",
      "2.5432963371276855\n",
      "2.8129968643188477\n",
      "2.5674283504486084\n",
      "2.6891729831695557\n",
      "2.8135321140289307\n",
      "2.656362533569336\n",
      "2.635185956954956\n",
      "2.636131525039673\n",
      "2.4547393321990967\n",
      "2.716796875\n",
      "2.8460171222686768\n",
      "2.9388749599456787\n",
      "2.8181214332580566\n",
      "3.299067735671997\n",
      "2.50048828125\n",
      "2.8411765098571777\n",
      "2.515740394592285\n",
      "2.9432225227355957\n",
      "2.4486160278320312\n",
      "3.0285069942474365\n",
      "2.4320907592773438\n",
      "2.5130937099456787\n",
      "2.521634817123413\n",
      "2.61370849609375\n",
      "2.54129695892334\n",
      "2.663966655731201\n",
      "3.158313512802124\n",
      "2.8184781074523926\n",
      "2.871492624282837\n",
      "2.4213790893554688\n",
      "3.052980899810791\n",
      "2.964785575866699\n",
      "2.7651984691619873\n",
      "2.9360742568969727\n",
      "2.6218502521514893\n",
      "2.504965305328369\n",
      "2.636202573776245\n",
      "3.282543659210205\n",
      "2.6668453216552734\n",
      "3.0664947032928467\n",
      "2.5069897174835205\n",
      "3.1459004878997803\n",
      "3.0929410457611084\n",
      "2.710911750793457\n",
      "2.826585292816162\n",
      "2.5559070110321045\n",
      "3.0915791988372803\n",
      "2.501152753829956\n",
      "2.5631394386291504\n",
      "2.614304780960083\n",
      "2.4933838844299316\n",
      "3.121983051300049\n",
      "2.6546857357025146\n",
      "2.7265872955322266\n",
      "2.7214231491088867\n",
      "2.7034952640533447\n",
      "3.0034432411193848\n",
      "2.6072463989257812\n",
      "3.0225064754486084\n",
      "2.0369009971618652\n",
      "2.315419912338257\n",
      "2.4849867820739746\n",
      "2.8915679454803467\n",
      "3.0616233348846436\n",
      "2.380131244659424\n",
      "2.793335437774658\n",
      "2.9803378582000732\n",
      "2.200571298599243\n",
      "2.575507879257202\n",
      "2.984490394592285\n",
      "2.8122544288635254\n",
      "2.508944272994995\n",
      "2.650458574295044\n",
      "2.830913543701172\n",
      "3.0136704444885254\n",
      "2.5802605152130127\n",
      "2.9707446098327637\n",
      "3.031421184539795\n",
      "2.6796226501464844\n",
      "2.4358201026916504\n",
      "2.4575679302215576\n",
      "2.8154401779174805\n",
      "2.8442699909210205\n",
      "2.753528594970703\n",
      "2.887890100479126\n",
      "2.318115711212158\n",
      "2.955780506134033\n",
      "2.6008687019348145\n",
      "3.372887134552002\n",
      "2.5968077182769775\n",
      "2.6428768634796143\n",
      "2.708282947540283\n",
      "2.4146103858947754\n",
      "2.367839813232422\n",
      "2.485067844390869\n",
      "3.058539867401123\n",
      "2.5020997524261475\n",
      "2.3565802574157715\n",
      "2.828559637069702\n",
      "2.682396411895752\n",
      "2.8017449378967285\n",
      "3.088644027709961\n",
      "2.8940441608428955\n",
      "2.8381457328796387\n",
      "2.9579083919525146\n",
      "3.1724157333374023\n",
      "2.828446388244629\n",
      "2.772127151489258\n",
      "3.2830071449279785\n",
      "2.3890035152435303\n",
      "2.801632881164551\n",
      "2.712622880935669\n",
      "2.5746185779571533\n",
      "2.895960569381714\n",
      "2.851494550704956\n",
      "2.340254545211792\n",
      "2.61669921875\n",
      "2.501889705657959\n",
      "2.5972015857696533\n",
      "2.729745864868164\n",
      "2.45434308052063\n",
      "3.0059800148010254\n",
      "2.586804151535034\n",
      "2.6149024963378906\n",
      "2.243511438369751\n",
      "2.7821969985961914\n",
      "3.1733062267303467\n",
      "2.644659996032715\n",
      "2.5976226329803467\n",
      "2.471369504928589\n",
      "2.665915012359619\n",
      "2.62160587310791\n",
      "2.7168774604797363\n",
      "2.7658133506774902\n",
      "2.846748113632202\n",
      "2.9771580696105957\n",
      "3.1648313999176025\n",
      "2.5567052364349365\n",
      "2.69435977935791\n",
      "2.581582546234131\n",
      "2.416121244430542\n",
      "2.722930669784546\n",
      "2.8929615020751953\n",
      "2.6391539573669434\n",
      "2.758793830871582\n",
      "2.8078527450561523\n",
      "3.215698003768921\n",
      "2.8081581592559814\n",
      "2.740778684616089\n",
      "2.685865879058838\n",
      "2.6148183345794678\n",
      "2.736191987991333\n",
      "3.020735263824463\n",
      "2.640070676803589\n",
      "3.0695552825927734\n",
      "2.582519769668579\n",
      "2.926149606704712\n",
      "3.329897165298462\n",
      "2.668488025665283\n",
      "2.620396137237549\n",
      "2.4327902793884277\n",
      "2.5063583850860596\n",
      "2.554154872894287\n",
      "2.338632106781006\n",
      "2.366299867630005\n",
      "3.20226788520813\n",
      "2.9161064624786377\n",
      "2.7411653995513916\n",
      "2.829322338104248\n",
      "2.344440221786499\n",
      "2.3625144958496094\n",
      "2.498567819595337\n",
      "2.355015754699707\n",
      "2.699272871017456\n",
      "2.389965057373047\n",
      "3.094512701034546\n",
      "2.6106765270233154\n",
      "2.4780385494232178\n",
      "2.889139413833618\n",
      "2.344542980194092\n",
      "2.6915173530578613\n",
      "2.8578438758850098\n",
      "2.539393901824951\n",
      "3.242671489715576\n",
      "2.255514144897461\n",
      "2.520970344543457\n",
      "2.4382619857788086\n",
      "2.6370558738708496\n",
      "2.5339112281799316\n",
      "2.694664716720581\n",
      "2.697161912918091\n",
      "3.065356492996216\n",
      "2.743931770324707\n",
      "2.8776941299438477\n",
      "2.775719404220581\n",
      "2.777602195739746\n",
      "2.5983293056488037\n",
      "2.6341500282287598\n",
      "2.820272207260132\n",
      "3.274786949157715\n",
      "2.656531810760498\n",
      "2.3458545207977295\n",
      "2.8701510429382324\n",
      "3.1131744384765625\n",
      "2.749234914779663\n",
      "2.8108770847320557\n",
      "2.6679646968841553\n",
      "2.553774356842041\n",
      "2.8433914184570312\n",
      "2.5727310180664062\n",
      "2.557003974914551\n",
      "2.596057176589966\n",
      "3.2677390575408936\n",
      "2.442016363143921\n",
      "2.813014030456543\n",
      "3.388551712036133\n",
      "2.8047966957092285\n",
      "2.7011053562164307\n",
      "2.6687300205230713\n",
      "2.715656042098999\n",
      "2.7571768760681152\n",
      "2.57932448387146\n",
      "2.812084674835205\n",
      "2.49273943901062\n",
      "2.5748541355133057\n",
      "2.8538320064544678\n",
      "2.428798198699951\n",
      "3.075042963027954\n",
      "2.738880157470703\n",
      "2.812373638153076\n",
      "2.913874864578247\n",
      "3.003812551498413\n",
      "2.433317184448242\n",
      "2.647467851638794\n",
      "3.2839932441711426\n",
      "2.567842483520508\n",
      "2.21525502204895\n",
      "2.601850748062134\n",
      "3.273543119430542\n",
      "2.820133686065674\n",
      "2.5287537574768066\n",
      "2.6765995025634766\n",
      "2.5507187843322754\n",
      "2.463231086730957\n",
      "2.7657203674316406\n",
      "3.0512661933898926\n",
      "3.112466812133789\n",
      "2.518247365951538\n",
      "2.613168954849243\n",
      "2.6160125732421875\n",
      "2.6027021408081055\n",
      "2.9110825061798096\n",
      "2.9423904418945312\n",
      "2.981982707977295\n",
      "2.3722801208496094\n",
      "2.939504861831665\n",
      "2.722074270248413\n",
      "3.116828203201294\n",
      "2.4077210426330566\n",
      "3.2288320064544678\n",
      "2.871882915496826\n",
      "2.683626651763916\n",
      "2.3327810764312744\n",
      "2.583846092224121\n",
      "3.2913756370544434\n",
      "2.608289957046509\n",
      "2.877916097640991\n",
      "2.6434812545776367\n",
      "3.3873302936553955\n",
      "2.542202949523926\n",
      "2.586756467819214\n",
      "3.162341833114624\n",
      "2.724022626876831\n",
      "2.3398756980895996\n",
      "2.8380749225616455\n",
      "2.4447731971740723\n",
      "2.508995294570923\n",
      "3.284214496612549\n",
      "2.571565866470337\n",
      "2.7325658798217773\n",
      "2.180872678756714\n",
      "2.6451075077056885\n",
      "2.6659955978393555\n",
      "2.6573851108551025\n",
      "2.3098645210266113\n",
      "2.8676209449768066\n",
      "2.6466476917266846\n",
      "2.955596446990967\n",
      "2.8116393089294434\n",
      "2.6424813270568848\n",
      "2.679913282394409\n",
      "2.542290449142456\n",
      "2.5671796798706055\n",
      "2.7803869247436523\n",
      "2.512094259262085\n",
      "2.4210472106933594\n",
      "2.7970738410949707\n",
      "2.832653522491455\n",
      "2.3550093173980713\n",
      "2.8415279388427734\n",
      "2.7867157459259033\n",
      "2.4998350143432617\n",
      "2.5261518955230713\n",
      "2.4478747844696045\n",
      "2.59702730178833\n",
      "2.752753973007202\n",
      "2.812110662460327\n",
      "2.355839729309082\n",
      "2.514953851699829\n",
      "2.9345099925994873\n",
      "2.880455732345581\n",
      "2.9087491035461426\n",
      "2.802924633026123\n",
      "2.827146053314209\n",
      "3.201967716217041\n",
      "2.782304286956787\n",
      "2.5009517669677734\n",
      "2.6530542373657227\n",
      "2.8166537284851074\n",
      "2.5463757514953613\n",
      "2.6277008056640625\n",
      "2.446974992752075\n",
      "3.0749049186706543\n",
      "2.6263458728790283\n",
      "2.686861038208008\n",
      "2.7199206352233887\n",
      "2.6115355491638184\n",
      "2.6831436157226562\n",
      "2.5208330154418945\n",
      "2.404360055923462\n",
      "3.160029888153076\n",
      "3.299760103225708\n",
      "2.9119293689727783\n",
      "2.8007664680480957\n",
      "2.699798822402954\n",
      "2.464323043823242\n",
      "2.4482028484344482\n",
      "2.700683832168579\n",
      "2.7953028678894043\n",
      "2.502957820892334\n",
      "2.4595086574554443\n",
      "2.7468016147613525\n",
      "2.811750888824463\n",
      "2.7481634616851807\n",
      "2.352797508239746\n",
      "2.6490397453308105\n",
      "2.5352792739868164\n",
      "2.6010360717773438\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "\n",
    "    # Minibatch construction\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # Last parameter specifies the dimensions of the returned tensor, here (32,) means a 1D vector\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2), indexing into X, which is the full training set, and grabbing 32 random samples\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)  # (32, 100)\n",
    "    logits = h @ W2 + b2  # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # print(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:    \n",
    "        p.grad = None   # IMPORTANT Note: Need to reset the gradients every training run.\n",
    "                        # Otherwise the model would think the error is much larger than it actually is, \n",
    "                        # because it's adding errors from previous batches or iterations together, which is not relevant to the current iteration's\n",
    "                        # loss!\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "print(loss.item()) # final batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2519, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final forward pass to check total loss:\n",
    "\n",
    "emb = C[X[ix]] # (32, 3, 2), indexing into X, which is the full training set, and grabbing 32 random samples\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)  # (32, 100)\n",
    "logits = h @ W2 + b2  # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y[ix])\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
