{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation based on the following paper:\n",
    "\n",
    "# https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key concept: Words are embedded in an n-dimensional embedding space. The MLP learns to organize this space in a way that forms understanding of\n",
    "#              semantics. The goal is to give similar embeddings to similar words. Each embedding dimensions will encode the model's compressed\n",
    "#              understanding. This allows the model to transfer knowledge and generalize to novel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoi = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "itos = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # {} to create a dictionary, i+1 so the starting index is 1, enumerate returns an iterable of tuples\n",
    "print(f'{stoi = }')\n",
    "\n",
    "# Syntax is: {key_expression: value_expression for item in iterable}\n",
    "\n",
    "stoi['.'] = 0 # Add start/end token with 0 index\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(f'{itos = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'),\n",
       " (1, 'b'),\n",
       " (2, 'c'),\n",
       " (3, 'd'),\n",
       " (4, 'e'),\n",
       " (5, 'f'),\n",
       " (6, 'g'),\n",
       " (7, 'h'),\n",
       " (8, 'i'),\n",
       " (9, 'j'),\n",
       " (10, 'k'),\n",
       " (11, 'l'),\n",
       " (12, 'm'),\n",
       " (13, 'n'),\n",
       " (14, 'o'),\n",
       " (15, 'p'),\n",
       " (16, 'q'),\n",
       " (17, 'r'),\n",
       " (18, 's'),\n",
       " (19, 't'),\n",
       " (20, 'u'),\n",
       " (21, 'v'),\n",
       " (22, 'w'),\n",
       " (23, 'x'),\n",
       " (24, 'y'),\n",
       " (25, 'z')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "...  ---->  e\n",
      "..e  ---->  m\n",
      ".em  ---->  m\n",
      "emm  ---->  a\n",
      "mma  ---->  .\n",
      "olivia\n",
      "...  ---->  o\n",
      "..o  ---->  l\n",
      ".ol  ---->  i\n",
      "oli  ---->  v\n",
      "liv  ---->  i\n",
      "ivi  ---->  a\n",
      "via  ---->  .\n",
      "ava\n",
      "...  ---->  a\n",
      "..a  ---->  v\n",
      ".av  ---->  a\n",
      "ava  ---->  .\n",
      "isabella\n",
      "...  ---->  i\n",
      "..i  ---->  s\n",
      ".is  ---->  a\n",
      "isa  ---->  b\n",
      "sab  ---->  e\n",
      "abe  ---->  l\n",
      "bel  ---->  l\n",
      "ell  ---->  a\n",
      "lla  ---->  .\n",
      "sophia\n",
      "...  ---->  s\n",
      "..s  ---->  o\n",
      ".so  ---->  p\n",
      "sop  ---->  h\n",
      "oph  ---->  i\n",
      "phi  ---->  a\n",
      "hia  ---->  .\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset\n",
    "\n",
    "block_size = 3 # Context length\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size # Multiplication of a list with a positive integer replicates the contents of the list that many times. \n",
    "                               # here [0, 0, 0]\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), ' ----> ', itos[ix])\n",
    "        \n",
    "        context = context[1:] + [ix] # Crop and append. This essentially shifts the context window to the right.\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape = torch.Size([27, 2]), \n",
      "C.dtype = torch.float32, \n",
      "C = tensor([[-0.3624, -0.4966],\n",
      "        [-1.2292,  0.1354],\n",
      "        [ 0.7915, -0.2870],\n",
      "        [ 0.2531, -1.1512],\n",
      "        [ 0.1504, -2.2786],\n",
      "        [-1.8882,  0.2456],\n",
      "        [ 2.1242,  0.9396],\n",
      "        [-1.4602, -1.1785],\n",
      "        [ 0.6620,  0.2064],\n",
      "        [-1.7284, -0.0120],\n",
      "        [-1.6734, -1.3556],\n",
      "        [-0.7919, -1.3280],\n",
      "        [ 0.3607, -0.3684],\n",
      "        [ 1.3874, -0.6661],\n",
      "        [-1.2694, -0.9046],\n",
      "        [-0.5850,  0.1864],\n",
      "        [-1.0960,  0.5380],\n",
      "        [ 0.2761,  0.2254],\n",
      "        [ 0.2845,  1.1659],\n",
      "        [ 2.5072,  1.6047],\n",
      "        [ 0.5344, -1.1039],\n",
      "        [ 0.4764, -0.5298],\n",
      "        [-0.7484, -0.3143],\n",
      "        [-0.2986, -0.3627],\n",
      "        [ 0.8337,  0.0834],\n",
      "        [ 0.0124, -0.5235],\n",
      "        [-0.2294, -0.5578]])\n"
     ]
    }
   ],
   "source": [
    "# Create the lookup table \"C\" of embeddings per character\n",
    "\n",
    "C = torch.randn((27,2)) # 27 characters embedded in a 2-dimensional space. Returns a tensor.\n",
    "print(f'{C.shape = }, \\n{C.dtype = }, \\n{C = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape # We can index into the tensor C using another tensor.\n",
    "\n",
    "# The shape is (sample x character x embedding)\n",
    "# Remember 1 \"sample\" is made of 3 characters (the preceding 3, to be exact) at that is why the first two dimensions are 32 x 3,\n",
    "# and \"embedding \" consists of 2 values, which is why the last dimension is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C[0] = tensor([-0.3624, -0.4966])\n",
      "C[X][0,:,0] = tensor([-0.3624, -0.3624, -0.3624])\n"
     ]
    }
   ],
   "source": [
    "# Explanation of pytorch advanced indexing\n",
    "\n",
    "# What Happens in C[X]?\n",
    "# When you do C[X], you're not directly indexing C as if C were 3D. Instead, you're using the advanced indexing feature in PyTorch, \n",
    "# where the tensor X acts as a lookup table for rows in C. Here's how it works:\n",
    "\n",
    "# Indexing Mechanism:\n",
    "# X specifies which rows of C to select.\n",
    "# Each ENTRY of X is an index that tells PyTorch which row of C to pick.\n",
    "\n",
    "print(f'{C[0] = }') # This selects the first row of C\n",
    "print(f'{C[X][0,:,0] = }') # This grabs the first embedding for each character in the first sample (sample = context, made of 3 characters)\n",
    "                           # So the 3rd dimension is the useful one which contains the embeddings for each character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hidden layer\n",
    "\n",
    "# Matrix of weights\n",
    "\n",
    "W1 = torch.randn((6, 100))  # The NN takes the previous 3 characters and predicts the next one,\n",
    "                            # and each character was embedded using 2 values (2D embedding space).\n",
    "                            # This means that the NN takes (3 characters * 2 embeddings/character = 6 values),\n",
    "                            # and we set the layer to have 100 neurons.\n",
    "                        \n",
    "b1 = torch.randn(100)       # Make 100 biases, 1 per neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-1.8882,  0.2456]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-1.8882,  0.2456],\n",
       "         [ 1.3874, -0.6661]],\n",
       "\n",
       "        [[-1.8882,  0.2456],\n",
       "         [ 1.3874, -0.6661],\n",
       "         [ 1.3874, -0.6661]],\n",
       "\n",
       "        [[ 1.3874, -0.6661],\n",
       "         [ 1.3874, -0.6661],\n",
       "         [-1.2292,  0.1354]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-0.5850,  0.1864]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.5850,  0.1864],\n",
       "         [ 0.3607, -0.3684]],\n",
       "\n",
       "        [[-0.5850,  0.1864],\n",
       "         [ 0.3607, -0.3684],\n",
       "         [-1.7284, -0.0120]],\n",
       "\n",
       "        [[ 0.3607, -0.3684],\n",
       "         [-1.7284, -0.0120],\n",
       "         [-0.7484, -0.3143]],\n",
       "\n",
       "        [[-1.7284, -0.0120],\n",
       "         [-0.7484, -0.3143],\n",
       "         [-1.7284, -0.0120]],\n",
       "\n",
       "        [[-0.7484, -0.3143],\n",
       "         [-1.7284, -0.0120],\n",
       "         [-1.2292,  0.1354]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-1.2292,  0.1354]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-1.2292,  0.1354],\n",
       "         [-0.7484, -0.3143]],\n",
       "\n",
       "        [[-1.2292,  0.1354],\n",
       "         [-0.7484, -0.3143],\n",
       "         [-1.2292,  0.1354]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-1.7284, -0.0120]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-1.7284, -0.0120],\n",
       "         [ 2.5072,  1.6047]],\n",
       "\n",
       "        [[-1.7284, -0.0120],\n",
       "         [ 2.5072,  1.6047],\n",
       "         [-1.2292,  0.1354]],\n",
       "\n",
       "        [[ 2.5072,  1.6047],\n",
       "         [-1.2292,  0.1354],\n",
       "         [ 0.7915, -0.2870]],\n",
       "\n",
       "        [[-1.2292,  0.1354],\n",
       "         [ 0.7915, -0.2870],\n",
       "         [-1.8882,  0.2456]],\n",
       "\n",
       "        [[ 0.7915, -0.2870],\n",
       "         [-1.8882,  0.2456],\n",
       "         [ 0.3607, -0.3684]],\n",
       "\n",
       "        [[-1.8882,  0.2456],\n",
       "         [ 0.3607, -0.3684],\n",
       "         [ 0.3607, -0.3684]],\n",
       "\n",
       "        [[ 0.3607, -0.3684],\n",
       "         [ 0.3607, -0.3684],\n",
       "         [-1.2292,  0.1354]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [-0.3624, -0.4966],\n",
       "         [ 2.5072,  1.6047]],\n",
       "\n",
       "        [[-0.3624, -0.4966],\n",
       "         [ 2.5072,  1.6047],\n",
       "         [-0.5850,  0.1864]],\n",
       "\n",
       "        [[ 2.5072,  1.6047],\n",
       "         [-0.5850,  0.1864],\n",
       "         [-1.0960,  0.5380]],\n",
       "\n",
       "        [[-0.5850,  0.1864],\n",
       "         [-1.0960,  0.5380],\n",
       "         [ 0.6620,  0.2064]],\n",
       "\n",
       "        [[-1.0960,  0.5380],\n",
       "         [ 0.6620,  0.2064],\n",
       "         [-1.7284, -0.0120]],\n",
       "\n",
       "        [[ 0.6620,  0.2064],\n",
       "         [-1.7284, -0.0120],\n",
       "         [-1.2292,  0.1354]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.8882,  0.2456],\n",
       "        [-0.3624, -0.4966, -1.8882,  0.2456,  1.3874, -0.6661],\n",
       "        [-1.8882,  0.2456,  1.3874, -0.6661,  1.3874, -0.6661],\n",
       "        [ 1.3874, -0.6661,  1.3874, -0.6661, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.5850,  0.1864],\n",
       "        [-0.3624, -0.4966, -0.5850,  0.1864,  0.3607, -0.3684],\n",
       "        [-0.5850,  0.1864,  0.3607, -0.3684, -1.7284, -0.0120],\n",
       "        [ 0.3607, -0.3684, -1.7284, -0.0120, -0.7484, -0.3143],\n",
       "        [-1.7284, -0.0120, -0.7484, -0.3143, -1.7284, -0.0120],\n",
       "        [-0.7484, -0.3143, -1.7284, -0.0120, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -1.2292,  0.1354, -0.7484, -0.3143],\n",
       "        [-1.2292,  0.1354, -0.7484, -0.3143, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.7284, -0.0120],\n",
       "        [-0.3624, -0.4966, -1.7284, -0.0120,  2.5072,  1.6047],\n",
       "        [-1.7284, -0.0120,  2.5072,  1.6047, -1.2292,  0.1354],\n",
       "        [ 2.5072,  1.6047, -1.2292,  0.1354,  0.7915, -0.2870],\n",
       "        [-1.2292,  0.1354,  0.7915, -0.2870, -1.8882,  0.2456],\n",
       "        [ 0.7915, -0.2870, -1.8882,  0.2456,  0.3607, -0.3684],\n",
       "        [-1.8882,  0.2456,  0.3607, -0.3684,  0.3607, -0.3684],\n",
       "        [ 0.3607, -0.3684,  0.3607, -0.3684, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966,  2.5072,  1.6047],\n",
       "        [-0.3624, -0.4966,  2.5072,  1.6047, -0.5850,  0.1864],\n",
       "        [ 2.5072,  1.6047, -0.5850,  0.1864, -1.0960,  0.5380],\n",
       "        [-0.5850,  0.1864, -1.0960,  0.5380,  0.6620,  0.2064],\n",
       "        [-1.0960,  0.5380,  0.6620,  0.2064, -1.7284, -0.0120],\n",
       "        [ 0.6620,  0.2064, -1.7284, -0.0120, -1.2292,  0.1354]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, the layer has to perform a matrix multiplication to operate on the inputs:\n",
    "\n",
    "# Something like emb @ W1 + b1\n",
    "\n",
    "# But this can't be done because the dimensions of emb and W1 are not compatible. Therefore, we need to reshape emb e.g. through concatenation.\n",
    "\n",
    "\n",
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)    # This grabs the embeddings of the 1st char, 2nd char, and 3rd char\n",
    "                                                            # and adds them as columns (dim 1)\n",
    "\n",
    "# The problem is that this code does not generalize because we have to manually list the tensors in the list that is passed to cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.8882,  0.2456],\n",
       "        [-0.3624, -0.4966, -1.8882,  0.2456,  1.3874, -0.6661],\n",
       "        [-1.8882,  0.2456,  1.3874, -0.6661,  1.3874, -0.6661],\n",
       "        [ 1.3874, -0.6661,  1.3874, -0.6661, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.5850,  0.1864],\n",
       "        [-0.3624, -0.4966, -0.5850,  0.1864,  0.3607, -0.3684],\n",
       "        [-0.5850,  0.1864,  0.3607, -0.3684, -1.7284, -0.0120],\n",
       "        [ 0.3607, -0.3684, -1.7284, -0.0120, -0.7484, -0.3143],\n",
       "        [-1.7284, -0.0120, -0.7484, -0.3143, -1.7284, -0.0120],\n",
       "        [-0.7484, -0.3143, -1.7284, -0.0120, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -1.2292,  0.1354, -0.7484, -0.3143],\n",
       "        [-1.2292,  0.1354, -0.7484, -0.3143, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.7284, -0.0120],\n",
       "        [-0.3624, -0.4966, -1.7284, -0.0120,  2.5072,  1.6047],\n",
       "        [-1.7284, -0.0120,  2.5072,  1.6047, -1.2292,  0.1354],\n",
       "        [ 2.5072,  1.6047, -1.2292,  0.1354,  0.7915, -0.2870],\n",
       "        [-1.2292,  0.1354,  0.7915, -0.2870, -1.8882,  0.2456],\n",
       "        [ 0.7915, -0.2870, -1.8882,  0.2456,  0.3607, -0.3684],\n",
       "        [-1.8882,  0.2456,  0.3607, -0.3684,  0.3607, -0.3684],\n",
       "        [ 0.3607, -0.3684,  0.3607, -0.3684, -1.2292,  0.1354],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
       "        [-0.3624, -0.4966, -0.3624, -0.4966,  2.5072,  1.6047],\n",
       "        [-0.3624, -0.4966,  2.5072,  1.6047, -0.5850,  0.1864],\n",
       "        [ 2.5072,  1.6047, -0.5850,  0.1864, -1.0960,  0.5380],\n",
       "        [-0.5850,  0.1864, -1.0960,  0.5380,  0.6620,  0.2064],\n",
       "        [-1.0960,  0.5380,  0.6620,  0.2064, -1.7284, -0.0120],\n",
       "        [ 0.6620,  0.2064, -1.7284, -0.0120, -1.2292,  0.1354]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better way with torch.unbind(emb, 1), which is equal to [emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]] but generalizes.\n",
    "\n",
    "torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]), \n",
      "a.shape = torch.Size([18]), \n",
      "\n",
      "a.storage() =  0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      " 12\n",
      " 13\n",
      " 14\n",
      " 15\n",
      " 16\n",
      " 17\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emman\\AppData\\Local\\Temp\\ipykernel_5844\\3278925131.py:6: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f'{a = }, \\n{a.shape = }, \\n\\n{a.storage() = }')\n"
     ]
    }
   ],
   "source": [
    "# But there is actually a more efficient way.\n",
    "\n",
    "# Proof:\n",
    "\n",
    "a = torch.arange(18)\n",
    "print(f'{a = }, \\n{a.shape = }, \\n\\n{a.storage() = }') # Note: a.storage() returns a TypedStorage object, which is deprecated. Use .untyped_storage()\n",
    "\n",
    "# Note: In computer memory the data of a tensor is always represented as a 1D vector!\n",
    "#       Pytorch then interprets this number sequence as a tensor of specific dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.view(3,3,2) = tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]]])\n",
      "\n",
      "emb.view(32, 6) = tensor([[-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.8882,  0.2456],\n",
      "        [-0.3624, -0.4966, -1.8882,  0.2456,  1.3874, -0.6661],\n",
      "        [-1.8882,  0.2456,  1.3874, -0.6661,  1.3874, -0.6661],\n",
      "        [ 1.3874, -0.6661,  1.3874, -0.6661, -1.2292,  0.1354],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.5850,  0.1864],\n",
      "        [-0.3624, -0.4966, -0.5850,  0.1864,  0.3607, -0.3684],\n",
      "        [-0.5850,  0.1864,  0.3607, -0.3684, -1.7284, -0.0120],\n",
      "        [ 0.3607, -0.3684, -1.7284, -0.0120, -0.7484, -0.3143],\n",
      "        [-1.7284, -0.0120, -0.7484, -0.3143, -1.7284, -0.0120],\n",
      "        [-0.7484, -0.3143, -1.7284, -0.0120, -1.2292,  0.1354],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.2292,  0.1354],\n",
      "        [-0.3624, -0.4966, -1.2292,  0.1354, -0.7484, -0.3143],\n",
      "        [-1.2292,  0.1354, -0.7484, -0.3143, -1.2292,  0.1354],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -1.7284, -0.0120],\n",
      "        [-0.3624, -0.4966, -1.7284, -0.0120,  2.5072,  1.6047],\n",
      "        [-1.7284, -0.0120,  2.5072,  1.6047, -1.2292,  0.1354],\n",
      "        [ 2.5072,  1.6047, -1.2292,  0.1354,  0.7915, -0.2870],\n",
      "        [-1.2292,  0.1354,  0.7915, -0.2870, -1.8882,  0.2456],\n",
      "        [ 0.7915, -0.2870, -1.8882,  0.2456,  0.3607, -0.3684],\n",
      "        [-1.8882,  0.2456,  0.3607, -0.3684,  0.3607, -0.3684],\n",
      "        [ 0.3607, -0.3684,  0.3607, -0.3684, -1.2292,  0.1354],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966, -0.3624, -0.4966],\n",
      "        [-0.3624, -0.4966, -0.3624, -0.4966,  2.5072,  1.6047],\n",
      "        [-0.3624, -0.4966,  2.5072,  1.6047, -0.5850,  0.1864],\n",
      "        [ 2.5072,  1.6047, -0.5850,  0.1864, -1.0960,  0.5380],\n",
      "        [-0.5850,  0.1864, -1.0960,  0.5380,  0.6620,  0.2064],\n",
      "        [-1.0960,  0.5380,  0.6620,  0.2064, -1.7284, -0.0120],\n",
      "        [ 0.6620,  0.2064, -1.7284, -0.0120, -1.2292,  0.1354]])\n"
     ]
    }
   ],
   "source": [
    "# On the other hand,\n",
    "\n",
    "print(f'{a.view(3,3,2) = }') # a.view(9, 9), a.view(6, 3), etc.\n",
    "\n",
    "#  .view() is therefore more efficient than .cat() in PyTorch because .view() only alters the shape of the tensor by modifying its metadata, \n",
    "#  without changing the underlying data. This operation is computationally inexpensive since no new memory allocation occurs. \n",
    "#  In contrast, .cat() creates a new tensor by concatenating existing tensors, which involves copying data and allocating new memory, \n",
    "#  making it more resource-intensive.\n",
    "\n",
    "# So instead we can do:\n",
    "\n",
    "print(f'\\n{emb.view(32, 6) = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1129, -2.6930,  1.4483,  ...,  0.9303,  0.6698,  0.1502],\n",
       "        [-0.2639, -2.2789,  2.2134,  ...,  0.3233,  1.9377,  0.0224],\n",
       "        [ 3.6410, -4.4634, -0.0771,  ...,  2.3118,  0.3872,  2.2425],\n",
       "        ...,\n",
       "        [ 1.3664, -2.8027,  0.3359,  ...,  1.9897,  0.5240,  3.0480],\n",
       "        [ 0.1678, -1.4718,  1.6818,  ..., -0.2888,  1.5553,  0.0467],\n",
       "        [-0.7058, -0.3824,  0.1012,  ...,  1.1909,  1.8748,  0.3961]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform hidden layer operation\n",
    "\n",
    "# emb.view(32, 6) @ W1 + b1\n",
    "emb.view(emb.shape[0], 6) @ W1 + b1     # Even better so dims are not hardcoded.\n",
    "                                        # emb.shape[0] selects the first dimension given by emb.shape.\n",
    "                                        # Can also use -1 as the first dim so Pytorch derives it itself, knowing that the number of elements has to\n",
    "                                        # be equal to the original tensor's.\n",
    "\n",
    "# Corresponds to the logits of the hidden layer, pre-activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = tensor([[ 0.9712, -0.9909,  0.8954,  ...,  0.7307,  0.5849,  0.1491],\n",
      "        [-0.2580, -0.9792,  0.9764,  ...,  0.3125,  0.9594,  0.0224],\n",
      "        [ 0.9986, -0.9997, -0.0769,  ...,  0.9806,  0.3689,  0.9777],\n",
      "        ...,\n",
      "        [ 0.8779, -0.9927,  0.3238,  ...,  0.9633,  0.4808,  0.9955],\n",
      "        [ 0.1662, -0.8999,  0.9331,  ..., -0.2810,  0.9147,  0.0466],\n",
      "        [-0.6080, -0.3648,  0.1009,  ...,  0.8309,  0.9540,  0.3766]]), \n",
      "\n",
      "h.shape = torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "# Calculate activations\n",
    "\n",
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "\n",
    "print(f'{h = }, \\n\\n{h.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(emb.view(emb.shape[0], 6) @ W1).shape = torch.Size([32, 100])\n",
      "\n",
      "b1.shape = torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting correctness check:\n",
    "\n",
    "print(f'{(emb.view(emb.shape[0], 6) @ W1).shape = }')\n",
    "print(f'\\n{b1.shape = }')\n",
    "\n",
    "# So internally the broadcasting will\n",
    "\n",
    "# 32, 100\n",
    "#     100\n",
    "\n",
    "# first align the dimensions from the right,\n",
    "\n",
    "# 32, 100\n",
    "#  1, 100\n",
    "\n",
    "# then add a dimension made of 1s entries (this turns b1 into a row vector),\n",
    "# and then this row vector will be copied vertically to match the 32, 100 dim necessary for the addition\n",
    "\n",
    "# Conclusion: broadcasting in this case achieves the desired operation as the element-wise addition is performed with the same values for every row,\n",
    "# meaning each neuron (dim 1) adds the same bias to each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
